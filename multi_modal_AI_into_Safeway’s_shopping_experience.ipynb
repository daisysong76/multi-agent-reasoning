{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZAlxGjMS4XNdxSYusY2xr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisysong76/multi-agent-reasoning/blob/main/multi_modal_AI_into_Safeway%E2%80%99s_shopping_experience.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A high-level implementation of various advanced components that Safeway could use to create a multi-modal AI system for their personalized shopping experience"
      ],
      "metadata": {
        "id": "Qfrzo3agHMV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Data Collection and Integration\n",
        "\n",
        "For this part, you will collect and process visual, text, speech, and behavioral data.       ETL Pipeline"
      ],
      "metadata": {
        "id": "P1RNbQWSFyME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "import speech_recognition as sr\n",
        "\n",
        "# Extract and load behavioral data (example: CSV files)\n",
        "behavioral_data = pd.read_csv('behavioral_data.csv')\n",
        "\n",
        "# Load visual data (example: product images)\n",
        "def load_images(image_folder):\n",
        "    images = []\n",
        "    for img_name in os.listdir(image_folder):\n",
        "        img_path = os.path.join(image_folder, img_name)\n",
        "        img = Image.open(img_path)\n",
        "        images.append(img)\n",
        "    return images\n",
        "\n",
        "images = load_images('path_to_image_folder')\n",
        "\n",
        "# Speech to text (example: voice queries)\n",
        "recognizer = sr.Recognizer()\n",
        "with sr.AudioFile('customer_query.wav') as source:\n",
        "    audio_data = recognizer.record(source)\n",
        "    text_query = recognizer.recognize_google(audio_data)\n",
        "\n",
        "# Combine data into a unified structure\n",
        "combined_data = {\n",
        "    'behavior': behavioral_data,\n",
        "    'images': images,\n",
        "    'text_query': text_query\n",
        "}\n"
      ],
      "metadata": {
        "id": "VbjBvEA1F66b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Multi-Modal AI Model Development\n",
        "\n",
        "Visual Recommendation System (Using PyTorch)"
      ],
      "metadata": {
        "id": "vv1Pua3fGDMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "\n",
        "# Pretrained ResNet model for image features\n",
        "class VisualRecommendationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VisualRecommendationModel, self).__init__()\n",
        "        self.base_model = models.resnet50(pretrained=True)\n",
        "        self.fc = nn.Linear(self.base_model.fc.in_features, 128)  # Feature embeddings of size 128\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Load image data\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "def process_images(image_list):\n",
        "    return torch.stack([image_transform(image) for image in image_list])\n",
        "\n",
        "model = VisualRecommendationModel()\n",
        "images_tensor = process_images(images)\n",
        "visual_embeddings = model(images_tensor)\n"
      ],
      "metadata": {
        "id": "Olv0jt3AGD-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text and Speech Processing with NLP (Using GPT-4-like Transformer Model"
      ],
      "metadata": {
        "id": "pIO4FE5MGKdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Initialize GPT-2 or GPT-4-like model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Process customer text query\n",
        "inputs = tokenizer(text_query, return_tensors='pt')\n",
        "outputs = model.generate(inputs['input_ids'], max_length=100)\n",
        "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(f'Processed text query response: {decoded_output}')\n"
      ],
      "metadata": {
        "id": "Dg5S2i6fGL3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Behavioral Data Analysis (Using RNN)"
      ],
      "metadata": {
        "id": "qapSH4tPGOJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BehavioralModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(BehavioralModel, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, _ = self.rnn(x)\n",
        "        out = self.fc(h[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Example: behavioral data in the form of a time-series\n",
        "behavioral_data_tensor = torch.tensor(behavioral_data.values, dtype=torch.float32)\n",
        "\n",
        "# Behavioral Model\n",
        "behavior_model = BehavioralModel(input_size=10, hidden_size=64, output_size=128)\n",
        "behavioral_embedding = behavior_model(behavioral_data_tensor.unsqueeze(0))\n"
      ],
      "metadata": {
        "id": "rlBYLjXVGQlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Unified Embedding Space for Cross-Modal Recommendations\n",
        "\n",
        "Using a pre-trained CLIP model for combining visual and text data into a unified embedding space."
      ],
      "metadata": {
        "id": "U8MdrZIvGU9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# Initialize CLIP model and processor\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Process visual and text data for unified embeddings\n",
        "inputs = clip_processor(text=[text_query], images=images, return_tensors=\"pt\", padding=True)\n",
        "outputs = clip_model(**inputs)\n",
        "\n",
        "visual_embedding = outputs.image_embeds\n",
        "text_embedding = outputs.text_embeds\n",
        "\n",
        "# Combine visual and text embeddings in joint space\n",
        "combined_embeddings = torch.cat((visual_embedding, text_embedding), dim=1)\n"
      ],
      "metadata": {
        "id": "BsRNX32SGX1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Contextual and Personalized Recommendations with Bandit Algorithm"
      ],
      "metadata": {
        "id": "wMcNYECJGbFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class ContextualBandit:\n",
        "    def __init__(self, n_actions):\n",
        "        self.n_actions = n_actions\n",
        "        self.q_values = np.zeros(n_actions)\n",
        "        self.action_count = np.zeros(n_actions)\n",
        "\n",
        "    def select_action(self):\n",
        "        return np.argmax(self.q_values + np.sqrt(2 * np.log(np.sum(self.action_count) + 1) / (self.action_count + 1e-10)))\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        self.action_count[action] += 1\n",
        "        self.q_values[action] += (reward - self.q_values[action]) / self.action_count[action]\n",
        "\n",
        "# Example: Assuming 5 products\n",
        "bandit = ContextualBandit(n_actions=5)\n",
        "selected_action = bandit.select_action()\n",
        "\n",
        "# Update based on customer interaction (reward = 1 if product purchased)\n",
        "bandit.update(selected_action, reward=1)\n"
      ],
      "metadata": {
        "id": "BFu-8C1bGdV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Augmented Reality and Visual Search\n",
        "(Using OpenCV + TensorFlow)"
      ],
      "metadata": {
        "id": "5nr5VNh0Gggz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load a pre-trained object detection model (e.g., MobileNet SSD)\n",
        "model = tf.saved_model.load('ssd_mobilenet_v2_fpnlite')\n",
        "\n",
        "# AR Product Detection\n",
        "def detect_product_in_frame(frame):\n",
        "    input_tensor = tf.convert_to_tensor([frame], dtype=tf.float32)\n",
        "    detections = model(input_tensor)\n",
        "    return detections['detection_boxes'], detections['detection_scores']\n",
        "\n",
        "# Capture video frame\n",
        "cap = cv2.VideoCapture(0)\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Detect products in the frame\n",
        "    boxes, scores = detect_product_in_frame(frame)\n",
        "\n",
        "    # Display detection on screen\n",
        "    for box, score in zip(boxes, scores):\n",
        "        if score > 0.5:\n",
        "            cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)\n",
        "\n",
        "    cv2.imshow('AR Product Detection', frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "Y2jAuqOAGi1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Deployment with Docker and Kubernetes\n",
        "Dockerfile"
      ],
      "metadata": {
        "id": "r8Su0iFaGmJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use an official Python runtime as a base image\n",
        "FROM python:3.9-slim\n",
        "\n",
        "# Set the working directory in the container\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy the current directory contents into the container\n",
        "COPY . /app\n",
        "\n",
        "# Install any needed packages\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Run the application\n",
        "CMD [\"python\", \"app.py\"]\n"
      ],
      "metadata": {
        "id": "1hGpNR-yG_6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kubernetes Deployment"
      ],
      "metadata": {
        "id": "JV7OPTbGG3p8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apiVersion: apps/v1\n",
        "kind: Deployment\n",
        "metadata:\n",
        "  name: safeway-ml-deployment\n",
        "spec:\n",
        "  replicas: 3\n",
        "  selector:\n",
        "    matchLabels:\n",
        "      app: safeway-ml\n",
        "  template:\n",
        "    metadata:\n",
        "      labels:\n",
        "        app: safeway-ml\n",
        "    spec:\n",
        "      containers:\n",
        "      - name: safeway-ml-container\n",
        "        image: safeway-ml:latest\n",
        "        ports:\n",
        "        - containerPort: 5000\n"
      ],
      "metadata": {
        "id": "CRd2zWdEHDdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To develop the most advanced and promising approach for integrating multi-modal AI into Safeway’s shopping experience, here’s a detailed strategy that maximizes personalization and enhances the customer journey:\n",
        "\n",
        "1. Data Collection and Integration\n",
        "Visual Data: Capture product images from catalogs, online listings, and in-store displays. Ensure that these images are consistently tagged with relevant metadata such as product categories, ingredients, and price.\n",
        "Text and Speech Data: Collect customer queries from online chatbots, voice interactions (e.g., via Alexa), product reviews, and feedback forms. Safeway could also analyze text descriptions and nutritional labels.\n",
        "Behavioral Data: Monitor real-time customer behavior, such as browsing patterns, in-store movements, purchase history, shopping frequency, and preferred product categories.\n",
        "Advanced Approach:\n",
        "\n",
        "Use an ETL pipeline (Extract, Transform, Load) to process these data streams into a unified data lake, leveraging cloud services like AWS S3 or Google Cloud Storage. This ensures all data types—visual, text, and behavioral—are organized for analysis and model training.\n",
        "2. Multi-Modal AI Model Development\n",
        "Build a multi-modal AI system that processes these different data types together to make holistic recommendations:\n",
        "\n",
        "Visual Recommendation System:\n",
        "Use Convolutional Neural Networks (CNNs) or Vision Transformers to process product images and create feature embeddings. These embeddings allow the system to understand product similarities visually, such as colors, shapes, and packaging designs.\n",
        "Fine-tune models to recommend visually similar products (e.g., matching wine with cheese, or suggesting side dishes with a main course).\n",
        "Text and Speech Processing with NLP:\n",
        "Leverage transformer models like GPT-4 or BERT to understand customer queries, product descriptions, and feedback. Fine-tune these models on Safeway’s specific domain for accurate interpretation of customer preferences.\n",
        "Speech-to-Text Models (e.g., Whisper) can be used to transcribe customer voice queries in real time, enabling Safeway to use NLP for advanced interaction in voice assistants or chatbots.\n",
        "Behavioral Data Analysis:\n",
        "Utilize Recurrent Neural Networks (RNNs) or Temporal Convolutional Networks (TCNs) to analyze real-time shopping behavior and sequence patterns. This helps predict customer preferences based on past shopping behavior.\n",
        "Incorporate reinforcement learning to adjust real-time recommendations dynamically, offering promotions or alternative suggestions based on live behavior in the app or in-store.\n",
        "3. Unified Embedding Space for Cross-Modal Recommendations\n",
        "Develop a joint embedding space where visual, textual, and behavioral data can be projected into the same latent space. This allows the system to make recommendations across modalities.\n",
        "For example, a customer might take a picture of a product in the store, and the system can recommend a product based on that image, plus their previous shopping history and voice inputs.\n",
        "Implementation:\n",
        "\n",
        "Use CLIP (Contrastive Language–Image Pretraining) or similar multi-modal models to link visual data (product images) with text descriptions (e.g., product ingredients or categories).\n",
        "Combine self-supervised learning to train these models across Safeway’s entire product inventory and shopping data, enabling the system to make more intuitive and context-aware recommendations.\n",
        "4. Contextual and Personalized Recommendations\n",
        "Use contextual bandit algorithms to deliver real-time personalized promotions based on both short-term behavior (current session data) and long-term behavior (historical data). This allows Safeway to continuously refine recommendations as more data is collected.\n",
        "\n",
        "Example: If a customer frequently buys organic products and browses health-related items, the system can prioritize organic or health-conscious products in future recommendations.\n",
        "\n",
        "Advanced Optimization:\n",
        "\n",
        "Implement Graph Neural Networks (GNNs) to map relationships between products and customers. This is especially useful for modeling complex interactions between multiple products (e.g., pairings like wine and cheese or complementary items like pasta and sauce).\n",
        "Use reinforcement learning to optimize product placements and recommendation flows based on how customers respond to recommendations over time.\n",
        "5. Seamless Customer Experience with AR and AI\n",
        "Augmented Reality (AR): Use AR for an immersive in-store experience where customers can point their smartphone cameras at products, and the AI system provides real-time recommendations for complementary items or promotions.\n",
        "Combine AR with AI-based visual search to help customers quickly locate products or get recipe ideas based on what’s in their shopping cart or pantry.\n",
        "Example: If a customer scans a jar of pasta sauce, the system could recommend pasta, garlic bread, and a bottle of wine based on their preferences, past purchases, and current deals.\n",
        "\n",
        "6. Deployment and Real-Time Inference\n",
        "Use edge computing for real-time AI inference in-store, allowing the Safeway app or in-store kiosks to provide immediate feedback and suggestions.\n",
        "Ensure the models are deployed in a containerized environment (e.g., Docker or Kubernetes) to allow seamless updates and scalability. Safeway can use AWS Lambda or Google Cloud Functions for serverless architecture to handle peaks in traffic during promotions or sales.\n",
        "7. Continuous Learning and Improvement\n",
        "Implement A/B testing and multi-armed bandits to experiment with different recommendation strategies, continuously optimizing the model to increase customer engagement and sales.\n",
        "Use feedback loops to improve recommendations: If a customer ignores or rejects certain suggestions, the system adjusts future recommendations accordingly.\n",
        "8. Privacy and Security with Federated Learning\n",
        "Safeway could deploy federated learning to ensure that customer data remains private and compliant with privacy laws (e.g., GDPR). This allows the AI models to be trained locally on customer devices without sharing sensitive data, while still improving the overall system.\n",
        "Architecture Summary:\n",
        "Data Collection: Real-time capture of visual, text, speech, and behavioral data.\n",
        "Multi-Modal AI Models: Joint embedding of image, text, and behavior data using CLIP or Vision Transformers, and NLP models like GPT-4.\n",
        "Contextual Bandits: Continuous real-time recommendation adjustments based on customer interactions.\n",
        "AR Integration: Enhance in-store experience using visual search and AR-driven product discovery.\n",
        "Real-Time Inference: Edge computing and cloud-based deployments for scalable and efficient processing.\n",
        "Continuous Improvement: Federated learning and A/B testing for ongoing model refinement and privacy-preserving learning.\n",
        "By implementing this multi-modal AI architecture, Safeway can create a truly personalized and engaging shopping experience that maximizes convenience, boosts sales, and keeps customers coming back with tailored, real-time recommendations."
      ],
      "metadata": {
        "id": "xTaTUlGmFspD"
      }
    }
  ]
}